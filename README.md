# LLM-Response-Quality-Evaluator
A lightweight tool that evaluates LLM responses against a user prompt using a rubric-based framework. It scores relevance, completeness, clarity, and hallucination risk, producing structured feedback to simulate how enterprise AI teams assess reliability and usability of generated outputs.
